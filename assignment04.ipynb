{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment02.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PatrickP-Student/-enel645Team25Assignment4/blob/model-davis/assignment04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a90a8sE_abWS"
      },
      "source": [
        "# Assignment 04 - Garbage Bin Classification problem \n",
        "\n",
        "\n",
        "This assignment is the continuation of assignment 01, where the teams were tasked with designing a garbage classification system.\n",
        "\n",
        "For the first assignmnet 4,068 images were collected. With the following distribution:\n",
        "\n",
        "- **Blue bin** (recyclable): 2,398 images\n",
        "- **Green bin** (compostable):      826 images\n",
        "- **Black bin** (landfill):         844 images \n",
        "\n",
        "\n",
        "For this assignment, your team needs to develop/implement/code the garbage classification model. You are free to use any technique seen in class that you want (*e.g.*, CNNs, transfer learning, etc.). You will have access only to the development set. The TAs will run your code on the test set to extract the accuracy and confusion matrix metrics.\n",
        "\n",
        "The development set can be downloaded here:\n",
        "\n",
        "- [OneDrive](https://uofc-my.sharepoint.com/:u:/g/personal/roberto_medeirosdeso_ucalgary_ca/EYEMTmqSm9RGodAIQDKB5lwBp2xyWtNm8qQ0wj7JV2XiPA?e=1xhhDh) - Link expires March 10th, 2021.\n",
        "- [GDrive](https://drive.google.com/file/d/1-q56xKd4yEsFo5xwz5Rd1Zn_lyCXLaMU/view?usp=sharing)\n",
        "\n",
        "The data has already been pre-processed for you. Images were resized to 512 x 400 pixels and converted to PNG. Be mindful that a considerable number of samples in the development set may have been incorrectly labelled. Your team is free to fix some of the labels if you think this will help to develop your model. [See what goes where](https://www.calgary.ca/uep/wrs/what-goes-where/default.html) to get information about the labels.\n",
        "\n",
        "\n",
        "The Jupyter Notebook should be divided into two parts: 1. Model development; 2. Model testing. The model development will be run by you, while the model testing will be run by the TAs when grading the assignment.\n",
        "\n",
        "\n",
        "The deliverables for this assignment are:\n",
        "\n",
        "1. This jupyter-notebook completed with your solution. \n",
        "    - Name the notebook as enel645_assignment04_team_(team number).ipynb\n",
        "2. The weights of your best model after training. \n",
        "    - Name the weights' file as team_(team number)_garbage.h5 \n",
        "\n",
        "\n",
        "Submit the two files (notebook + models' weights) to your dropbox on the course D2L page.\n",
        "    \n",
        "You are free to add extra cells of text and code to this notebook. You are free to use the TALC cluster to train your model, but remember that your code should be submitted as a Jupyter Notebook and not \".py\" file.\n",
        "\n",
        "Please include a short description of what each team member did in the assignment at the end of the notebook. Also include the consensus score between 0 and 3 for each team member. This score will be used to adjust the final grade of each student. Students developing the project individually do not need this description and score.\n",
        "\n",
        "You are being assessed based on:\n",
        "\n",
        "1. Code execution - 20% \n",
        "2. Clarity of the code (e.g., easy to follow, has pertinent comments, etc.) - 20%\n",
        "3. Proper usage of the techniques seen in class - 30% \n",
        "4. Accuracy of the models  - 30%\n",
        "\n",
        "\n",
        "## 1. Model development;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBR9mwL5aX6r"
      },
      "source": [
        "# Develop your model here feel free to add additional cells\n",
        "# Comment and justify your choices as much as you can."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOF4qRoUrFPi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01d832a2-7a2f-448f-9092-ff25f5e1f269"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxWZdMstoIJR"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m6NAMYMqKGk"
      },
      "source": [
        "# specify the paths to the training and validation datasets\n",
        "train_path = '/content/drive/My Drive/Colab Notebooks/Dataset/Train'\n",
        "val_path = '/content/drive/My Drive/Colab Notebooks/Dataset/Val'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEkC1K2rsS0i"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# specify our model constant settings\n",
        "batch_size = 32\n",
        "seed = 17\n",
        "input_size_3d = (512, 400, 3)\n",
        "input_size_2d = (512, 400)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAcPXVNNsJRe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5d3422d-bba2-4798-ec28-6066d5010128"
      },
      "source": [
        "# augment, load, and encode our training data set classes\n",
        "train_data_gen = ImageDataGenerator(rotation_range=20, \n",
        "                                    horizontal_flip=True, \n",
        "                                    height_shift_range=0.1)\n",
        "\n",
        "train_generator = train_data_gen.flow_from_directory(train_path,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    seed=seed,\n",
        "                                                    target_size=input_size_2d,\n",
        "                                                    class_mode='categorical')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3391 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XOQ68bc4z0q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98575ce7-d2ed-4552-e7f8-f3b172ec56db"
      },
      "source": [
        "# augment, load, and encode our validation data set classes\n",
        "val_data_gen = ImageDataGenerator(rotation_range=20, \n",
        "                                  horizontal_flip=True, \n",
        "                                  height_shift_range=0.1)\n",
        "\n",
        "val_generator = train_data_gen.flow_from_directory(val_path,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  seed=seed,\n",
        "                                                  target_size=input_size_2d,\n",
        "                                                  class_mode='categorical')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 377 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM2IIpFtoW0F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2116baf5-b1b6-4659-b3f0-90865f26e388"
      },
      "source": [
        "# lets look at our data in terms of image shape, number of channels, and dimensions\n",
        "print('train classes:',train_generator.class_indices)\n",
        "print('validation classes:',val_generator.class_indices)\n",
        "\n",
        "h, w, r = train_generator.image_shape\n",
        "print('There are', train_generator.samples, 'images for training the model')\n",
        "print('~', round(train_generator.samples/train_generator.num_classes,0), 'images per category')\n",
        "print('The shape of each image is', train_generator.image_shape)\n",
        "print('The width is', w)\n",
        "print('The height is', h)\n",
        "print('And each pixel has a value for each component of RGB for a total of', r, 'channels')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train classes: {'black': 0, 'blue': 1, 'green': 2}\n",
            "validation classes: {'black': 0, 'blue': 1, 'green': 2}\n",
            "There are 3391 images for training the model\n",
            "~ 1130.0 images per category\n",
            "The shape of each image is (512, 400, 3)\n",
            "The width is 400\n",
            "The height is 512\n",
            "And each pixel has a value for each component of RGB for a total of 3 channels\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taf90AKy6Oc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fe71794-5c29-4725-c4f9-e8d80d4f3593"
      },
      "source": [
        "# lets look at our train validation split\n",
        "num_train_samples = int(train_generator.samples)\n",
        "num_val_samples = int(val_generator.samples)\n",
        "\n",
        "print(num_train_samples, 'images in the train data set')\n",
        "print(num_val_samples, 'images in the validation data set')\n",
        "print('Split is, train:', round(num_train_samples/(num_train_samples + num_val_samples), 2)*100, '% validation:', round(num_val_samples/(num_train_samples + num_val_samples), 2)*100, '%')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3391 images in the train data set\n",
            "377 images in the validation data set\n",
            "Split is, train: 90.0 % validation: 10.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGnUGM1YcjQr"
      },
      "source": [
        "### 1.2 Model Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LODKgr67chNc"
      },
      "source": [
        "model_name = \"Xception_garbageBin_cnn.h5\"\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 20)\n",
        "\n",
        "monitor = tf.keras.callbacks.ModelCheckpoint(model_name, monitor='val_loss',\\\n",
        "                                             verbose=0,save_best_only=True,\\\n",
        "                                             save_weights_only=True,\\\n",
        "                                             mode='min')\n",
        "# Learning rate schedule\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch%4 == 0 and epoch!= 0:\n",
        "        lr = lr/2\n",
        "    return lr\n",
        "\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler,verbose = 0)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZybyJ5CZct12"
      },
      "source": [
        "### 1.3 Transfer Learning with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81xE9jKJcq7M"
      },
      "source": [
        "# Transfer learning using DenseNet121 model\n",
        "base_model = tf.keras.applications.Xception(\n",
        "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
        "    input_shape=input_size_3d,\n",
        "    include_top=False) \n",
        "\n",
        "base_model.trainable = False"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfcRq1R8eI0x"
      },
      "source": [
        "### 1.4 Adding Prediction Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cM0nfOINeBFD",
        "outputId": "73f7e488-51db-4bb0-8672-70de1eabc1be"
      },
      "source": [
        "input_image = tf.keras.Input(shape=input_size_3d)\n",
        "x1 = base_model(input_image, training=False)\n",
        "x2 = tf.keras.layers.Flatten()(x1)\n",
        "out = tf.keras.layers.Dense(3,activation='softmax')(x2)\n",
        "model = tf.keras.Model(inputs = input_image, outputs=out)\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_12 (InputLayer)        [(None, 512, 400, 3)]     0         \n",
            "_________________________________________________________________\n",
            "xception (Functional)        (None, 16, 13, 2048)      20861480  \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 425984)            0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 1277955   \n",
            "=================================================================\n",
            "Total params: 22,139,435\n",
            "Trainable params: 1,277,955\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IkSRWwcCBAU"
      },
      "source": [
        "### 1.5 Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdPk2PMgeBnC",
        "outputId": "ad99352c-63e9-4493-b257-c097ff571ab7"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr = 1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_generator, steps_per_epoch=3391/batch_size,\n",
        "          epochs=20, verbose=1,\n",
        "          callbacks=[early_stop, monitor, lr_schedule],\n",
        "          validation_data=val_generator, validation_steps=377/batch_size)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "105/105 [==============================] - 197s 2s/step - loss: 50.6043 - accuracy: 0.5181 - val_loss: 31.1963 - val_accuracy: 0.5942\n",
            "Epoch 2/20\n",
            "105/105 [==============================] - 191s 2s/step - loss: 23.4883 - accuracy: 0.5989 - val_loss: 43.2042 - val_accuracy: 0.6260\n",
            "Epoch 3/20\n",
            "105/105 [==============================] - 190s 2s/step - loss: 23.2607 - accuracy: 0.6355 - val_loss: 34.0160 - val_accuracy: 0.6393\n",
            "Epoch 4/20\n",
            "105/105 [==============================] - 189s 2s/step - loss: 30.4654 - accuracy: 0.6279 - val_loss: 26.4963 - val_accuracy: 0.5570\n",
            "Epoch 5/20\n",
            "105/105 [==============================] - 188s 2s/step - loss: 13.7981 - accuracy: 0.6840 - val_loss: 19.2290 - val_accuracy: 0.6207\n",
            "Epoch 6/20\n",
            "105/105 [==============================] - 188s 2s/step - loss: 11.7806 - accuracy: 0.6945 - val_loss: 17.8978 - val_accuracy: 0.5517\n",
            "Epoch 7/20\n",
            "105/105 [==============================] - 188s 2s/step - loss: 10.0790 - accuracy: 0.7031 - val_loss: 18.8439 - val_accuracy: 0.6340\n",
            "Epoch 8/20\n",
            "105/105 [==============================] - 188s 2s/step - loss: 9.2929 - accuracy: 0.7245 - val_loss: 23.0535 - val_accuracy: 0.4987\n",
            "Epoch 9/20\n",
            "105/105 [==============================] - 187s 2s/step - loss: 11.1873 - accuracy: 0.6855 - val_loss: 12.9961 - val_accuracy: 0.5889\n",
            "Epoch 10/20\n",
            "105/105 [==============================] - 188s 2s/step - loss: 6.3551 - accuracy: 0.7705 - val_loss: 15.5086 - val_accuracy: 0.5729\n",
            "Epoch 11/20\n",
            "105/105 [==============================] - 191s 2s/step - loss: 7.1796 - accuracy: 0.7212 - val_loss: 11.9670 - val_accuracy: 0.6233\n",
            "Epoch 12/20\n",
            "105/105 [==============================] - 188s 2s/step - loss: 6.4050 - accuracy: 0.7430 - val_loss: 10.4367 - val_accuracy: 0.6207\n",
            "Epoch 13/20\n",
            "105/105 [==============================] - 188s 2s/step - loss: 4.5218 - accuracy: 0.7773 - val_loss: 10.7378 - val_accuracy: 0.6472\n",
            "Epoch 14/20\n",
            "105/105 [==============================] - 187s 2s/step - loss: 4.3561 - accuracy: 0.7864 - val_loss: 8.7399 - val_accuracy: 0.6313\n",
            "Epoch 15/20\n",
            "105/105 [==============================] - 187s 2s/step - loss: 4.7569 - accuracy: 0.7710 - val_loss: 9.0025 - val_accuracy: 0.6472\n",
            "Epoch 16/20\n",
            "105/105 [==============================] - 187s 2s/step - loss: 4.0552 - accuracy: 0.7761 - val_loss: 8.5379 - val_accuracy: 0.6631\n",
            "Epoch 17/20\n",
            "105/105 [==============================] - 187s 2s/step - loss: 2.9925 - accuracy: 0.8107 - val_loss: 8.4993 - val_accuracy: 0.6207\n",
            "Epoch 18/20\n",
            "105/105 [==============================] - 187s 2s/step - loss: 3.6169 - accuracy: 0.7970 - val_loss: 10.3200 - val_accuracy: 0.6684\n",
            "Epoch 19/20\n",
            "105/105 [==============================] - 187s 2s/step - loss: 3.5745 - accuracy: 0.7996 - val_loss: 8.7167 - val_accuracy: 0.6605\n",
            "Epoch 20/20\n",
            "105/105 [==============================] - 187s 2s/step - loss: 3.0809 - accuracy: 0.8070 - val_loss: 8.3827 - val_accuracy: 0.6684\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7faf2cc85290>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY6v94A4Bq3U"
      },
      "source": [
        "### 1.6 Unfreeze base model and train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvQ84RZreDOU",
        "outputId": "32d60536-ff00-40d3-dca6-0da344ec9b79"
      },
      "source": [
        "base_model = tf.keras.applications.Xception(\n",
        "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
        "    input_shape=input_size_3d,\n",
        "    include_top=False) \n",
        "\n",
        "base_model.trainable = True\n",
        "\n",
        "input_image = tf.keras.Input(shape=input_size_3d)\n",
        "x1 = base_model(input_image, training=True)\n",
        "x2 = tf.keras.layers.Flatten()(x1)\n",
        "out = tf.keras.layers.Dense(3, activation='softmax')(x2)\n",
        "model = tf.keras.Model(inputs = input_image, outputs = out)\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_14 (InputLayer)        [(None, 512, 400, 3)]     0         \n",
            "_________________________________________________________________\n",
            "xception (Functional)        (None, 16, 13, 2048)      20861480  \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 425984)            0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 3)                 1277955   \n",
            "=================================================================\n",
            "Total params: 22,139,435\n",
            "Trainable params: 22,084,907\n",
            "Non-trainable params: 54,528\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "HVnCQJfICcd4",
        "outputId": "736d8f9d-07b8-46db-b09a-488b756b1910"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr = 1e-6), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.load_weights(model_name)\n",
        "\n",
        "model.fit(train_generator, steps_per_epoch=3391/batch_size,\n",
        "          epochs=100, verbose=1,\n",
        "          callbacks=[early_stop, monitor, lr_schedule],\n",
        "          validation_data=val_generator, validation_steps=377/batch_size)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-f51c787ba17d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m model.fit(train_generator, steps_per_epoch=3391/batch_size,\n\u001b[1;32m      5\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2232\u001b[0m             f, self.layers, skip_mismatch=skip_mismatch)\n\u001b[1;32m   2233\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2234\u001b[0;31m         \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2236\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    708\u001b[0m                        str(len(weight_values)) + ' elements.')\n\u001b[1;32m    709\u001b[0m     \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m   \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   3704\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3705\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3706\u001b[0;31m       \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3707\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3708\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m    889\u001b[0m             (\"Cannot assign to variable%s due to variable shape %s and value \"\n\u001b[1;32m    890\u001b[0m              \"shape %s are incompatible\") %\n\u001b[0;32m--> 891\u001b[0;31m             (tensor_name, self._shape, value_tensor.shape))\n\u001b[0m\u001b[1;32m    892\u001b[0m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[1;32m    893\u001b[0m           self.handle, value_tensor, name=name)\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot assign to variable block1_conv2/kernel:0 due to variable shape (3, 3, 32, 64) and value shape (32,) are incompatible"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCX59QIbn2z_"
      },
      "source": [
        "## 2. Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-1E5hhHn2z_"
      },
      "source": [
        "# You are free to adapt this portion of the code, but you should \n",
        "# compute the test accuracy and show the images that \n",
        "# were classified incorrectly\n",
        "test_data_dir = \"/media/roberto/f5da97cf-b92d-484c-96e9-15766931cebe/Garbage-classification/Dataset-curated/Resized/Test/\"\n",
        "\n",
        "model.load_weights(model_name)\n",
        "\n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1/255.0) \n",
        "test_generator = test_datagen.flow_from_directory(test_data_dir,\n",
        "                                                    target_size=(img_height, img_width),\n",
        "                                                    batch_size= batch_size,shuffle = False)\n",
        "nb_samples = len(glob.glob(test_data_dir + \"*/*\"))\n",
        "model.evaluate(test_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-4jTI1zn20A"
      },
      "source": [
        "img = []\n",
        "true_label = []\n",
        "pred_label = []\n",
        "for ii in range(nb_samples//batch_size):\n",
        "    Xbatch,Ybatch = test_generator.__getitem__(ii)\n",
        "    Ybatch = Ybatch.argmax(axis = 1)\n",
        "    Ypred = model.predict(Xbatch).argmax(axis = 1)\n",
        "    wrong_indexes = np.where(Ypred != Ybatch)[0]\n",
        "    for ii in wrong_indexes:\n",
        "        img.append(Xbatch[ii])\n",
        "        true_label.append(Ybatch[ii])\n",
        "        pred_label.append(Ypred[ii])\n",
        "\n",
        "columns = 4\n",
        "rows = len(img)//columns + 1    \n",
        "plt.figure(figsize = (32,64))\n",
        "for ii in range(len(img)):\n",
        "    plt.subplot(rows,columns,ii+1)\n",
        "    plt.imshow(img[ii], cmap = \"gray\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Label: %s, predicted: %ss\" %(class_names[true_label[ii]]\\\n",
        "                                            ,class_names[pred_label[ii]]))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjtJYZs5n20B"
      },
      "source": [
        "##  Team members participtaion\n",
        "(include the description of what each team member did and the consensus score for each team member)\n",
        "\n",
        "- **Arya Stark** helped design the model and write the code for fully connected model (**score 3**)\n",
        "- **Luke Skywalker** helped design helped to implement the data augmentation module (**score 3**)\n",
        "- ..."
      ]
    }
  ]
}