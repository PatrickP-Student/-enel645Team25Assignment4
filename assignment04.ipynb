{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment02.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PatrickP-Student/-enel645Team25Assignment4/blob/model-davis/assignment04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a90a8sE_abWS"
      },
      "source": [
        "# Assignment 04 - Garbage Bin Classification problem \n",
        "\n",
        "\n",
        "This assignment is the continuation of assignment 01, where the teams were tasked with designing a garbage classification system.\n",
        "\n",
        "For the first assignmnet 4,068 images were collected. With the following distribution:\n",
        "\n",
        "- **Blue bin** (recyclable): 2,398 images\n",
        "- **Green bin** (compostable):      826 images\n",
        "- **Black bin** (landfill):         844 images \n",
        "\n",
        "\n",
        "For this assignment, your team needs to develop/implement/code the garbage classification model. You are free to use any technique seen in class that you want (*e.g.*, CNNs, transfer learning, etc.). You will have access only to the development set. The TAs will run your code on the test set to extract the accuracy and confusion matrix metrics.\n",
        "\n",
        "The development set can be downloaded here:\n",
        "\n",
        "- [OneDrive](https://uofc-my.sharepoint.com/:u:/g/personal/roberto_medeirosdeso_ucalgary_ca/EYEMTmqSm9RGodAIQDKB5lwBp2xyWtNm8qQ0wj7JV2XiPA?e=1xhhDh) - Link expires March 10th, 2021.\n",
        "- [GDrive](https://drive.google.com/file/d/1-q56xKd4yEsFo5xwz5Rd1Zn_lyCXLaMU/view?usp=sharing)\n",
        "\n",
        "The data has already been pre-processed for you. Images were resized to 512 x 400 pixels and converted to PNG. Be mindful that a considerable number of samples in the development set may have been incorrectly labelled. Your team is free to fix some of the labels if you think this will help to develop your model. [See what goes where](https://www.calgary.ca/uep/wrs/what-goes-where/default.html) to get information about the labels.\n",
        "\n",
        "\n",
        "The Jupyter Notebook should be divided into two parts: 1. Model development; 2. Model testing. The model development will be run by you, while the model testing will be run by the TAs when grading the assignment.\n",
        "\n",
        "\n",
        "The deliverables for this assignment are:\n",
        "\n",
        "1. This jupyter-notebook completed with your solution. \n",
        "    - Name the notebook as enel645_assignment04_team_(team number).ipynb\n",
        "2. The weights of your best model after training. \n",
        "    - Name the weights' file as team_(team number)_garbage.h5 \n",
        "\n",
        "\n",
        "Submit the two files (notebook + models' weights) to your dropbox on the course D2L page.\n",
        "    \n",
        "You are free to add extra cells of text and code to this notebook. You are free to use the TALC cluster to train your model, but remember that your code should be submitted as a Jupyter Notebook and not \".py\" file.\n",
        "\n",
        "Please include a short description of what each team member did in the assignment at the end of the notebook. Also include the consensus score between 0 and 3 for each team member. This score will be used to adjust the final grade of each student. Students developing the project individually do not need this description and score.\n",
        "\n",
        "You are being assessed based on:\n",
        "\n",
        "1. Code execution - 20% \n",
        "2. Clarity of the code (e.g., easy to follow, has pertinent comments, etc.) - 20%\n",
        "3. Proper usage of the techniques seen in class - 30% \n",
        "4. Accuracy of the models  - 30%\n",
        "\n",
        "\n",
        "## 1. Model development;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBR9mwL5aX6r"
      },
      "source": [
        "# Develop your model here feel free to add additional cells\n",
        "# Comment and justify your choices as much as you can."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOF4qRoUrFPi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b92c4c5-b03e-4fa5-83ac-e841bd998f9e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxWZdMstoIJR"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m6NAMYMqKGk"
      },
      "source": [
        "# specify the paths to the training and validation datasets\n",
        "train_path = '/content/drive/My Drive/Colab Notebooks/Dataset/Train'\n",
        "val_path = '/content/drive/My Drive/Colab Notebooks/Dataset/Val'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEkC1K2rsS0i"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# specify our model constant settings\n",
        "batch_size = 32\n",
        "seed = 17\n",
        "input_size_3d = (512, 400, 3)\n",
        "input_size_2d = (512, 400)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAcPXVNNsJRe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "857d57cc-11a2-4bab-f391-1cc5fd46009a"
      },
      "source": [
        "# augment, load, and encode our training data set classes\n",
        "train_data_gen = ImageDataGenerator(rotation_range=20, \n",
        "                                    horizontal_flip=True, \n",
        "                                    height_shift_range=0.1)\n",
        "\n",
        "train_generator = train_data_gen.flow_from_directory(train_path,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    seed=seed,\n",
        "                                                    target_size=input_size_2d,\n",
        "                                                    class_mode='categorical')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3391 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XOQ68bc4z0q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "021ad99f-5baa-4a01-a1e2-6e80c72f2f1f"
      },
      "source": [
        "# augment, load, and encode our validation data set classes\n",
        "val_data_gen = ImageDataGenerator(rotation_range=20, \n",
        "                                  horizontal_flip=True, \n",
        "                                  height_shift_range=0.1)\n",
        "\n",
        "val_generator = train_data_gen.flow_from_directory(val_path,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  seed=seed,\n",
        "                                                  target_size=input_size_2d,\n",
        "                                                  class_mode='categorical')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 377 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM2IIpFtoW0F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21ce1d7a-8c51-4d17-b1e6-89a5dea70b8a"
      },
      "source": [
        "# lets look at our data in terms of image shape, number of channels, and dimensions\n",
        "print('train classes:',train_generator.class_indices)\n",
        "print('validation classes:',val_generator.class_indices)\n",
        "\n",
        "h, w, r = train_generator.image_shape\n",
        "print('There are', train_generator.samples, 'images for training the model')\n",
        "print('~', round(train_generator.samples/train_generator.num_classes,0), 'images per category')\n",
        "print('The shape of each image is', train_generator.image_shape)\n",
        "print('The width is', w)\n",
        "print('The height is', h)\n",
        "print('And each pixel has a value for each component of RGB for a total of', r, 'channels')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train classes: {'black': 0, 'blue': 1, 'green': 2}\n",
            "validation classes: {'black': 0, 'blue': 1, 'green': 2}\n",
            "There are 3391 images for training the model\n",
            "~ 1130.0 images per category\n",
            "The shape of each image is (512, 400, 3)\n",
            "The width is 400\n",
            "The height is 512\n",
            "And each pixel has a value for each component of RGB for a total of 3 channels\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taf90AKy6Oc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4486cd0-0abc-4df6-88ba-58b5cc30374c"
      },
      "source": [
        "# lets look at our train validation split\n",
        "num_train_samples = int(train_generator.samples)\n",
        "num_val_samples = int(val_generator.samples)\n",
        "\n",
        "print(num_train_samples, 'images in the train data set')\n",
        "print(num_val_samples, 'images in the validation data set')\n",
        "print('Split is, train:', round(num_train_samples/(num_train_samples + num_val_samples), 2)*100, '% validation:', round(num_val_samples/(num_train_samples + num_val_samples), 2)*100, '%')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3391 images in the train data set\n",
            "377 images in the validation data set\n",
            "Split is, train: 90.0 % validation: 10.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGnUGM1YcjQr"
      },
      "source": [
        "### 1.2 Model Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LODKgr67chNc"
      },
      "source": [
        "model_name = \"best_model_cnn.h5\"\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "monitor = tf.keras.callbacks.ModelCheckpoint(model_name, monitor = 'val_loss', verbose = 0, save_best_only = True, save_weights_only = True, mode = 'min')\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch%10 == 0:\n",
        "    lr = lr/2\n",
        "  return lr\n",
        "\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=0)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIQHc_50bgZZ"
      },
      "source": [
        "### 1.3 Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMPZ-U1RbgE_"
      },
      "source": [
        "def cnn_model(ishape = input_size_3d, k = 3, lr = 1e-4):\n",
        "\n",
        "  model_input = tf.keras.layers.Input(shape = ishape)\n",
        "\n",
        "  l1 = tf.keras.layers.Conv2D(32, (3,3), padding = 'same', activation = 'relu')(model_input)\n",
        "  l1_1 = tf.keras.layers.Conv2D(32, (3,3), padding = 'same', activation = 'relu')(l1)\n",
        "  l1_maxpool = tf.keras.layers.MaxPool2D((2,2))(l1_1)\n",
        "\n",
        "  l2 = tf.keras.layers.Conv2D(64, (3,3), padding = 'same', activation = 'relu')(l1_maxpool)\n",
        "  l2_1 = tf.keras.layers.Conv2D(64, (3,3), padding = 'same', activation = 'relu')(l2)\n",
        "  l2_maxpool = tf.keras.layers.MaxPool2D((2,2))(l2_1)\n",
        "\n",
        "  l3 = tf.keras.layers.Conv2D(128, (3,3), padding = 'same', activation = 'relu')(l2_maxpool)\n",
        "  l3_1 = tf.keras.layers.Conv2D(128, (3,3), padding = 'same', activation = 'relu')(l3)\n",
        "  l3_maxpool = tf.keras.layers.MaxPool2D((2,2))(l3_1)\n",
        "\n",
        "  l4 = tf.keras.layers.Conv2D(256, (3,3), padding = 'same', activation = 'relu')(l3_maxpool)\n",
        "  l4_1 = tf.keras.layers.Conv2D(256, (3,3), padding = 'same', activation = 'relu')(l4)\n",
        "  l4_maxpool = tf.keras.layers.MaxPool2D((2,2))(l4_1)\n",
        "\n",
        "  flat = tf.keras.layers.Flatten()(l4_maxpool)\n",
        "  flat_drop = tf.keras.layers.Dropout(0.5)(flat)\n",
        "  out = tf.keras.layers.Dense(k, activation = 'softmax')(flat_drop)\n",
        "\n",
        "  model = tf.keras.models.Model(inputs = model_input, outputs = out)\n",
        "  model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = lr), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tky9SDyicD7y"
      },
      "source": [
        "### 1.4 Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBJ6wLhJbzY6",
        "outputId": "9b7b5371-92b2-4a8f-acfe-b406fa164828",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = cnn_model()\n",
        "print(model.summary())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 512, 400, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 512, 400, 32)      896       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 512, 400, 32)      9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 256, 200, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 256, 200, 64)      18496     \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 256, 200, 64)      36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 128, 100, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 128, 100, 128)     73856     \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 128, 100, 128)     147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 64, 50, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 64, 50, 256)       295168    \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 64, 50, 256)       590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 32, 25, 256)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 204800)            0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 204800)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 3)                 614403    \n",
            "=================================================================\n",
            "Total params: 1,786,659\n",
            "Trainable params: 1,786,659\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IkSRWwcCBAU"
      },
      "source": [
        "### 1.6 Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdPk2PMgeBnC",
        "outputId": "5c90a0f7-859f-4357-ddbe-80cd76de097b"
      },
      "source": [
        "history = model.fit(train_generator, steps_per_epoch=3391/batch_size,\n",
        "          epochs=100, verbose=1,\n",
        "          callbacks=[early_stop, monitor, lr_schedule],\n",
        "          validation_data=val_generator, validation_steps=377/batch_size)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "105/105 [==============================] - 2875s 27s/step - loss: 1.7142 - accuracy: 0.5557 - val_loss: 0.8121 - val_accuracy: 0.6393\n",
            "Epoch 2/100\n",
            "105/105 [==============================] - 231s 2s/step - loss: 0.8343 - accuracy: 0.6346 - val_loss: 0.8065 - val_accuracy: 0.6366\n",
            "Epoch 3/100\n",
            "105/105 [==============================] - 233s 2s/step - loss: 0.7929 - accuracy: 0.6618 - val_loss: 0.7828 - val_accuracy: 0.6472\n",
            "Epoch 4/100\n",
            "105/105 [==============================] - 234s 2s/step - loss: 0.7865 - accuracy: 0.6530 - val_loss: 0.8067 - val_accuracy: 0.6605\n",
            "Epoch 5/100\n",
            "105/105 [==============================] - 236s 2s/step - loss: 0.7687 - accuracy: 0.6469 - val_loss: 0.7814 - val_accuracy: 0.6472\n",
            "Epoch 6/100\n",
            "105/105 [==============================] - 234s 2s/step - loss: 0.7288 - accuracy: 0.6794 - val_loss: 0.8090 - val_accuracy: 0.6366\n",
            "Epoch 7/100\n",
            "105/105 [==============================] - 234s 2s/step - loss: 0.7332 - accuracy: 0.6642 - val_loss: 0.8177 - val_accuracy: 0.6419\n",
            "Epoch 8/100\n",
            "105/105 [==============================] - 234s 2s/step - loss: 0.7024 - accuracy: 0.7018 - val_loss: 0.7792 - val_accuracy: 0.6472\n",
            "Epoch 9/100\n",
            "105/105 [==============================] - 232s 2s/step - loss: 0.7048 - accuracy: 0.7019 - val_loss: 0.7734 - val_accuracy: 0.6446\n",
            "Epoch 10/100\n",
            "105/105 [==============================] - 232s 2s/step - loss: 0.7002 - accuracy: 0.6909 - val_loss: 0.7731 - val_accuracy: 0.6525\n",
            "Epoch 11/100\n",
            "105/105 [==============================] - 234s 2s/step - loss: 0.6873 - accuracy: 0.7084 - val_loss: 0.7265 - val_accuracy: 0.6844\n",
            "Epoch 12/100\n",
            "105/105 [==============================] - 231s 2s/step - loss: 0.6754 - accuracy: 0.7041 - val_loss: 0.7880 - val_accuracy: 0.6446\n",
            "Epoch 13/100\n",
            "105/105 [==============================] - 233s 2s/step - loss: 0.6504 - accuracy: 0.7293 - val_loss: 0.7033 - val_accuracy: 0.6950\n",
            "Epoch 14/100\n",
            "105/105 [==============================] - 233s 2s/step - loss: 0.6438 - accuracy: 0.7163 - val_loss: 0.7333 - val_accuracy: 0.6923\n",
            "Epoch 15/100\n",
            "105/105 [==============================] - 233s 2s/step - loss: 0.6345 - accuracy: 0.7329 - val_loss: 0.7155 - val_accuracy: 0.6764\n",
            "Epoch 16/100\n",
            "105/105 [==============================] - 233s 2s/step - loss: 0.6183 - accuracy: 0.7420 - val_loss: 0.7186 - val_accuracy: 0.6923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ3bId7c25qL",
        "outputId": "359e2bfd-52a1-43a6-f882-b8c75da64efa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.load_weights(model_name)\n",
        "model.evaluate(val_generator)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 20s 2s/step - loss: 0.6932 - accuracy: 0.6817\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.693249523639679, 0.6816976070404053]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCX59QIbn2z_"
      },
      "source": [
        "## 2. Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-1E5hhHn2z_",
        "outputId": "100abfa4-c411-4174-b1a6-b6eaf5414172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "# You are free to adapt this portion of the code, but you should \n",
        "# compute the test accuracy and show the images that \n",
        "# were classified incorrectly\n",
        "test_data_dir = \"/media/roberto/f5da97cf-b92d-484c-96e9-15766931cebe/Garbage-classification/Dataset-curated/Resized/Test/\"\n",
        "\n",
        "model.load_weights(model_name)\n",
        "\n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1/255.0) \n",
        "test_generator = test_datagen.flow_from_directory(test_data_dir,\n",
        "                                                    target_size= input_size_2d,\n",
        "                                                    batch_size= batch_size,shuffle = False)\n",
        "nb_samples = len(glob.glob(test_data_dir + \"*/*\"))\n",
        "model.evaluate(test_generator)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-49426bfa774b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m test_generator = test_datagen.flow_from_directory(test_data_dir,\n\u001b[1;32m     10\u001b[0m                                                     \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0minput_size_2d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                                                     batch_size= batch_size,shuffle = False)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mnb_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"*/*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0mfollow_links\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_links\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 958\u001b[0;31m         interpolation=interpolation)\n\u001b[0m\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m   def flow_from_dataframe(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/roberto/f5da97cf-b92d-484c-96e9-15766931cebe/Garbage-classification/Dataset-curated/Resized/Test/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-4jTI1zn20A"
      },
      "source": [
        "img = []\n",
        "true_label = []\n",
        "pred_label = []\n",
        "for ii in range(nb_samples//batch_size):\n",
        "    Xbatch,Ybatch = test_generator.__getitem__(ii)\n",
        "    Ybatch = Ybatch.argmax(axis = 1)\n",
        "    Ypred = model.predict(Xbatch).argmax(axis = 1)\n",
        "    wrong_indexes = np.where(Ypred != Ybatch)[0]\n",
        "    for ii in wrong_indexes:\n",
        "        img.append(Xbatch[ii])\n",
        "        true_label.append(Ybatch[ii])\n",
        "        pred_label.append(Ypred[ii])\n",
        "\n",
        "columns = 4\n",
        "rows = len(img)//columns + 1    \n",
        "plt.figure(figsize = (32,64))\n",
        "for ii in range(len(img)):\n",
        "    plt.subplot(rows,columns,ii+1)\n",
        "    plt.imshow(img[ii], cmap = \"gray\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Label: %s, predicted: %ss\" %(class_names[true_label[ii]]\\\n",
        "                                            ,class_names[pred_label[ii]]))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjtJYZs5n20B"
      },
      "source": [
        "##  Team members participtaion\n",
        "(include the description of what each team member did and the consensus score for each team member)\n",
        "\n",
        "- **Arya Stark** helped design the model and write the code for fully connected model (**score 3**)\n",
        "- **Luke Skywalker** helped design helped to implement the data augmentation module (**score 3**)\n",
        "- ..."
      ]
    }
  ]
}