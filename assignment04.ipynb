{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment02.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PatrickP-Student/-enel645Team25Assignment4/blob/model-davis/assignment04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a90a8sE_abWS"
      },
      "source": [
        "# Assignment 04 - Garbage Bin Classification problem \n",
        "\n",
        "\n",
        "This assignment is the continuation of assignment 01, where the teams were tasked with designing a garbage classification system.\n",
        "\n",
        "For the first assignmnet 4,068 images were collected. With the following distribution:\n",
        "\n",
        "- **Blue bin** (recyclable): 2,398 images\n",
        "- **Green bin** (compostable):      826 images\n",
        "- **Black bin** (landfill):         844 images \n",
        "\n",
        "\n",
        "For this assignment, your team needs to develop/implement/code the garbage classification model. You are free to use any technique seen in class that you want (*e.g.*, CNNs, transfer learning, etc.). You will have access only to the development set. The TAs will run your code on the test set to extract the accuracy and confusion matrix metrics.\n",
        "\n",
        "The development set can be downloaded here:\n",
        "\n",
        "- [OneDrive](https://uofc-my.sharepoint.com/:u:/g/personal/roberto_medeirosdeso_ucalgary_ca/EYEMTmqSm9RGodAIQDKB5lwBp2xyWtNm8qQ0wj7JV2XiPA?e=1xhhDh) - Link expires March 10th, 2021.\n",
        "- [GDrive](https://drive.google.com/file/d/1-q56xKd4yEsFo5xwz5Rd1Zn_lyCXLaMU/view?usp=sharing)\n",
        "\n",
        "The data has already been pre-processed for you. Images were resized to 512 x 400 pixels and converted to PNG. Be mindful that a considerable number of samples in the development set may have been incorrectly labelled. Your team is free to fix some of the labels if you think this will help to develop your model. [See what goes where](https://www.calgary.ca/uep/wrs/what-goes-where/default.html) to get information about the labels.\n",
        "\n",
        "\n",
        "The Jupyter Notebook should be divided into two parts: 1. Model development; 2. Model testing. The model development will be run by you, while the model testing will be run by the TAs when grading the assignment.\n",
        "\n",
        "\n",
        "The deliverables for this assignment are:\n",
        "\n",
        "1. This jupyter-notebook completed with your solution. \n",
        "    - Name the notebook as enel645_assignment04_team_(team number).ipynb\n",
        "2. The weights of your best model after training. \n",
        "    - Name the weights' file as team_(team number)_garbage.h5 \n",
        "\n",
        "\n",
        "Submit the two files (notebook + models' weights) to your dropbox on the course D2L page.\n",
        "    \n",
        "You are free to add extra cells of text and code to this notebook. You are free to use the TALC cluster to train your model, but remember that your code should be submitted as a Jupyter Notebook and not \".py\" file.\n",
        "\n",
        "Please include a short description of what each team member did in the assignment at the end of the notebook. Also include the consensus score between 0 and 3 for each team member. This score will be used to adjust the final grade of each student. Students developing the project individually do not need this description and score.\n",
        "\n",
        "You are being assessed based on:\n",
        "\n",
        "1. Code execution - 20% \n",
        "2. Clarity of the code (e.g., easy to follow, has pertinent comments, etc.) - 20%\n",
        "3. Proper usage of the techniques seen in class - 30% \n",
        "4. Accuracy of the models  - 30%\n",
        "\n",
        "\n",
        "## 1. Model development;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBR9mwL5aX6r"
      },
      "source": [
        "# Develop your model here feel free to add additional cells\n",
        "# Comment and justify your choices as much as you can."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOF4qRoUrFPi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b90f9849-f14d-4d60-d6ae-68fbec384a8a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxWZdMstoIJR"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m6NAMYMqKGk"
      },
      "source": [
        "# specify the paths to the training and validation datasets\n",
        "train_path = '/content/drive/My Drive/Colab Notebooks/Dataset/Train'\n",
        "val_path = '/content/drive/My Drive/Colab Notebooks/Dataset/Val'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEkC1K2rsS0i"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# specify our model constant settings\n",
        "batch_size = 32\n",
        "seed = 17\n",
        "input_size_3d = (512, 400, 3)\n",
        "input_size_2d = (512, 400)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAcPXVNNsJRe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0948dbaf-e1c8-4bdb-f1e1-77a34fa3d6f2"
      },
      "source": [
        "# augment, load, and encode our training data set classes\n",
        "train_data_gen = ImageDataGenerator(rotation_range=20, \n",
        "                                    horizontal_flip=True, \n",
        "                                    height_shift_range=0.1)\n",
        "\n",
        "train_generator = train_data_gen.flow_from_directory(train_path,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    seed=seed,\n",
        "                                                    target_size=input_size_2d,\n",
        "                                                    class_mode='categorical')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3391 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XOQ68bc4z0q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21058b46-5ee6-49ee-ef01-e13b19ea7a8c"
      },
      "source": [
        "# augment, load, and encode our validation data set classes\n",
        "val_data_gen = ImageDataGenerator(rotation_range=20, \n",
        "                                  horizontal_flip=True, \n",
        "                                  height_shift_range=0.1)\n",
        "\n",
        "val_generator = train_data_gen.flow_from_directory(val_path,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  seed=seed,\n",
        "                                                  target_size=input_size_2d,\n",
        "                                                  class_mode='categorical')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 377 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM2IIpFtoW0F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e711ba0-8cbf-4bbb-db0a-aabda3684cf3"
      },
      "source": [
        "# lets look at our data in terms of image shape, number of channels, and dimensions\n",
        "print('train classes:',train_generator.class_indices)\n",
        "print('validation classes:',val_generator.class_indices)\n",
        "\n",
        "h, w, r = train_generator.image_shape\n",
        "print('There are', train_generator.samples, 'images for training the model')\n",
        "print('~', round(train_generator.samples/train_generator.num_classes,0), 'images per category')\n",
        "print('The shape of each image is', train_generator.image_shape)\n",
        "print('The width is', w)\n",
        "print('The height is', h)\n",
        "print('And each pixel has a value for each component of RGB for a total of', r, 'channels')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train classes: {'black': 0, 'blue': 1, 'green': 2}\n",
            "validation classes: {'black': 0, 'blue': 1, 'green': 2}\n",
            "There are 3391 images for training the model\n",
            "~ 1130.0 images per category\n",
            "The shape of each image is (512, 400, 3)\n",
            "The width is 400\n",
            "The height is 512\n",
            "And each pixel has a value for each component of RGB for a total of 3 channels\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taf90AKy6Oc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "964a5750-46c9-4338-868a-a65a5f856b6a"
      },
      "source": [
        "# lets look at our train validation split\n",
        "num_train_samples = int(train_generator.samples)\n",
        "num_val_samples = int(val_generator.samples)\n",
        "\n",
        "print(num_train_samples, 'images in the train data set')\n",
        "print(num_val_samples, 'images in the validation data set')\n",
        "print('Split is, train:', round(num_train_samples/(num_train_samples + num_val_samples), 2)*100, '% validation:', round(num_val_samples/(num_train_samples + num_val_samples), 2)*100, '%')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3391 images in the train data set\n",
            "377 images in the validation data set\n",
            "Split is, train: 90.0 % validation: 10.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGnUGM1YcjQr"
      },
      "source": [
        "### 1.2 Model Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LODKgr67chNc"
      },
      "source": [
        "model_name = \"densenet_garbageBin_cnn.h5\"\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 20)\n",
        "\n",
        "monitor = tf.keras.callbacks.ModelCheckpoint(model_name, monitor='val_loss',\\\n",
        "                                             verbose=0,save_best_only=True,\\\n",
        "                                             save_weights_only=True,\\\n",
        "                                             mode='min')\n",
        "# Learning rate schedule\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch%4 == 0 and epoch!= 0:\n",
        "        lr = lr/2\n",
        "    return lr\n",
        "\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler,verbose = 0)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZybyJ5CZct12"
      },
      "source": [
        "### 1.3 Transfer Learning with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81xE9jKJcq7M",
        "outputId": "e9bb1631-d0c5-4d72-a8d0-2d99e7be64f5"
      },
      "source": [
        "# Transfer learning using DenseNet121 model\n",
        "base_model = tf.keras.applications.DenseNet121(\n",
        "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
        "    input_shape=input_size_3d,\n",
        "    include_top=False) \n",
        "\n",
        "base_model.trainable = False"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29089792/29084464 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfcRq1R8eI0x"
      },
      "source": [
        "### 1.4 Adding Prediction Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cM0nfOINeBFD",
        "outputId": "ce721eab-4c57-4abc-d5bb-b17e3830e0c2"
      },
      "source": [
        "input_image = tf.keras.Input(shape=input_size_3d)\n",
        "x1 = base_model(input_image, training=False)\n",
        "x2 = tf.keras.layers.Flatten()(x1)\n",
        "out = tf.keras.layers.Dense(3, activation='softmax')(x2)\n",
        "model = tf.keras.Model(inputs = input_image, outputs = out)\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 512, 400, 3)]     0         \n",
            "_________________________________________________________________\n",
            "densenet121 (Functional)     (None, 16, 12, 1024)      7037504   \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 196608)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 3)                 589827    \n",
            "=================================================================\n",
            "Total params: 7,627,331\n",
            "Trainable params: 589,827\n",
            "Non-trainable params: 7,037,504\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IkSRWwcCBAU"
      },
      "source": [
        "### 1.5 Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdPk2PMgeBnC",
        "outputId": "fb2e488a-b7bf-42d1-f402-a6657ee61062"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr = 1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_generator, steps_per_epoch=3391/batch_size,\n",
        "          epochs=100, verbose=1,\n",
        "          callbacks=[early_stop, monitor, lr_schedule],\n",
        "          validation_data=val_generator, validation_steps=377/batch_size)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "105/105 [==============================] - 995s 9s/step - loss: 5.5424 - accuracy: 0.5785 - val_loss: 2.0027 - val_accuracy: 0.6790\n",
            "Epoch 2/100\n",
            "105/105 [==============================] - 178s 2s/step - loss: 2.2139 - accuracy: 0.6780 - val_loss: 3.1489 - val_accuracy: 0.6154\n",
            "Epoch 3/100\n",
            "105/105 [==============================] - 178s 2s/step - loss: 2.2794 - accuracy: 0.7135 - val_loss: 8.0435 - val_accuracy: 0.6525\n",
            "Epoch 4/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 3.6786 - accuracy: 0.6903 - val_loss: 2.4335 - val_accuracy: 0.6631\n",
            "Epoch 5/100\n",
            "105/105 [==============================] - 176s 2s/step - loss: 1.2355 - accuracy: 0.7894 - val_loss: 1.7587 - val_accuracy: 0.6950\n",
            "Epoch 6/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 1.0933 - accuracy: 0.7974 - val_loss: 2.0459 - val_accuracy: 0.7241\n",
            "Epoch 7/100\n",
            "105/105 [==============================] - 176s 2s/step - loss: 0.9676 - accuracy: 0.8126 - val_loss: 2.6934 - val_accuracy: 0.6207\n",
            "Epoch 8/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 1.2260 - accuracy: 0.7901 - val_loss: 3.4631 - val_accuracy: 0.7347\n",
            "Epoch 9/100\n",
            "105/105 [==============================] - 179s 2s/step - loss: 0.9985 - accuracy: 0.8099 - val_loss: 1.6793 - val_accuracy: 0.7082\n",
            "Epoch 10/100\n",
            "105/105 [==============================] - 179s 2s/step - loss: 0.7464 - accuracy: 0.8292 - val_loss: 1.4367 - val_accuracy: 0.7401\n",
            "Epoch 11/100\n",
            "105/105 [==============================] - 179s 2s/step - loss: 0.7233 - accuracy: 0.8340 - val_loss: 1.5103 - val_accuracy: 0.7321\n",
            "Epoch 12/100\n",
            "105/105 [==============================] - 178s 2s/step - loss: 0.5971 - accuracy: 0.8455 - val_loss: 1.6953 - val_accuracy: 0.7162\n",
            "Epoch 13/100\n",
            "105/105 [==============================] - 179s 2s/step - loss: 0.6097 - accuracy: 0.8431 - val_loss: 1.3728 - val_accuracy: 0.7480\n",
            "Epoch 14/100\n",
            "105/105 [==============================] - 181s 2s/step - loss: 0.5367 - accuracy: 0.8554 - val_loss: 1.2950 - val_accuracy: 0.7533\n",
            "Epoch 15/100\n",
            "105/105 [==============================] - 180s 2s/step - loss: 0.4706 - accuracy: 0.8666 - val_loss: 1.4375 - val_accuracy: 0.7135\n",
            "Epoch 16/100\n",
            "105/105 [==============================] - 179s 2s/step - loss: 0.5198 - accuracy: 0.8510 - val_loss: 1.4460 - val_accuracy: 0.7480\n",
            "Epoch 17/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 0.4223 - accuracy: 0.8804 - val_loss: 1.5830 - val_accuracy: 0.7745\n",
            "Epoch 18/100\n",
            "105/105 [==============================] - 178s 2s/step - loss: 0.4421 - accuracy: 0.8704 - val_loss: 1.4419 - val_accuracy: 0.7507\n",
            "Epoch 19/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 0.4060 - accuracy: 0.8801 - val_loss: 1.4010 - val_accuracy: 0.7294\n",
            "Epoch 20/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 0.4982 - accuracy: 0.8598 - val_loss: 1.3368 - val_accuracy: 0.7056\n",
            "Epoch 21/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 0.3728 - accuracy: 0.8852 - val_loss: 1.4637 - val_accuracy: 0.7427\n",
            "Epoch 22/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 0.3671 - accuracy: 0.8932 - val_loss: 1.4530 - val_accuracy: 0.7427\n",
            "Epoch 23/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 0.3796 - accuracy: 0.8866 - val_loss: 1.3983 - val_accuracy: 0.7215\n",
            "Epoch 24/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 0.3728 - accuracy: 0.8827 - val_loss: 1.2112 - val_accuracy: 0.7347\n",
            "Epoch 25/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 0.3843 - accuracy: 0.8901 - val_loss: 1.3524 - val_accuracy: 0.7162\n",
            "Epoch 26/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 0.3384 - accuracy: 0.8898 - val_loss: 1.4375 - val_accuracy: 0.7374\n",
            "Epoch 27/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 0.3047 - accuracy: 0.9041 - val_loss: 1.3555 - val_accuracy: 0.7294\n",
            "Epoch 28/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 0.3533 - accuracy: 0.9013 - val_loss: 1.4674 - val_accuracy: 0.7374\n",
            "Epoch 29/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 0.3497 - accuracy: 0.8919 - val_loss: 1.2842 - val_accuracy: 0.7427\n",
            "Epoch 30/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 0.3478 - accuracy: 0.8972 - val_loss: 1.2450 - val_accuracy: 0.7560\n",
            "Epoch 31/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 0.3483 - accuracy: 0.8955 - val_loss: 1.4635 - val_accuracy: 0.7162\n",
            "Epoch 32/100\n",
            "105/105 [==============================] - 178s 2s/step - loss: 0.3154 - accuracy: 0.9001 - val_loss: 1.2829 - val_accuracy: 0.7586\n",
            "Epoch 33/100\n",
            "105/105 [==============================] - 180s 2s/step - loss: 0.3257 - accuracy: 0.8942 - val_loss: 1.3738 - val_accuracy: 0.7294\n",
            "Epoch 34/100\n",
            "105/105 [==============================] - 180s 2s/step - loss: 0.3110 - accuracy: 0.8977 - val_loss: 1.2745 - val_accuracy: 0.7401\n",
            "Epoch 35/100\n",
            "105/105 [==============================] - 178s 2s/step - loss: 0.3418 - accuracy: 0.8886 - val_loss: 1.3483 - val_accuracy: 0.7162\n",
            "Epoch 36/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 0.3611 - accuracy: 0.8833 - val_loss: 1.3253 - val_accuracy: 0.7188\n",
            "Epoch 37/100\n",
            "105/105 [==============================] - 175s 2s/step - loss: 0.3631 - accuracy: 0.8787 - val_loss: 1.3095 - val_accuracy: 0.7321\n",
            "Epoch 38/100\n",
            "105/105 [==============================] - 176s 2s/step - loss: 0.2974 - accuracy: 0.8989 - val_loss: 1.2779 - val_accuracy: 0.7454\n",
            "Epoch 39/100\n",
            "105/105 [==============================] - 175s 2s/step - loss: 0.3427 - accuracy: 0.8877 - val_loss: 1.2260 - val_accuracy: 0.7401\n",
            "Epoch 40/100\n",
            "105/105 [==============================] - 175s 2s/step - loss: 0.3106 - accuracy: 0.9003 - val_loss: 1.1425 - val_accuracy: 0.7454\n",
            "Epoch 41/100\n",
            "105/105 [==============================] - 175s 2s/step - loss: 0.3002 - accuracy: 0.9002 - val_loss: 1.1028 - val_accuracy: 0.7586\n",
            "Epoch 42/100\n",
            "105/105 [==============================] - 175s 2s/step - loss: 0.3433 - accuracy: 0.8968 - val_loss: 1.2245 - val_accuracy: 0.7692\n",
            "Epoch 43/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 0.3226 - accuracy: 0.8974 - val_loss: 1.0719 - val_accuracy: 0.7560\n",
            "Epoch 44/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 0.3328 - accuracy: 0.8945 - val_loss: 1.1304 - val_accuracy: 0.7560\n",
            "Epoch 45/100\n",
            "105/105 [==============================] - 179s 2s/step - loss: 0.3069 - accuracy: 0.9016 - val_loss: 1.2179 - val_accuracy: 0.7586\n",
            "Epoch 46/100\n",
            "105/105 [==============================] - 178s 2s/step - loss: 0.2784 - accuracy: 0.9047 - val_loss: 1.2957 - val_accuracy: 0.7374\n",
            "Epoch 47/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 0.3549 - accuracy: 0.8957 - val_loss: 1.2564 - val_accuracy: 0.7401\n",
            "Epoch 48/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 0.3274 - accuracy: 0.8996 - val_loss: 1.1911 - val_accuracy: 0.7374\n",
            "Epoch 49/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 0.3306 - accuracy: 0.8921 - val_loss: 1.2251 - val_accuracy: 0.7454\n",
            "Epoch 50/100\n",
            "105/105 [==============================] - 179s 2s/step - loss: 0.3176 - accuracy: 0.8982 - val_loss: 1.1774 - val_accuracy: 0.7321\n",
            "Epoch 51/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 0.3349 - accuracy: 0.8999 - val_loss: 1.1697 - val_accuracy: 0.7347\n",
            "Epoch 52/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 0.2730 - accuracy: 0.9119 - val_loss: 1.2406 - val_accuracy: 0.7454\n",
            "Epoch 53/100\n",
            "105/105 [==============================] - 178s 2s/step - loss: 0.3391 - accuracy: 0.8985 - val_loss: 1.1687 - val_accuracy: 0.7241\n",
            "Epoch 54/100\n",
            "105/105 [==============================] - 178s 2s/step - loss: 0.3105 - accuracy: 0.8962 - val_loss: 1.2177 - val_accuracy: 0.7241\n",
            "Epoch 55/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 0.3247 - accuracy: 0.8938 - val_loss: 1.1486 - val_accuracy: 0.7454\n",
            "Epoch 56/100\n",
            "105/105 [==============================] - 179s 2s/step - loss: 0.3258 - accuracy: 0.8975 - val_loss: 1.3160 - val_accuracy: 0.7347\n",
            "Epoch 57/100\n",
            "105/105 [==============================] - 178s 2s/step - loss: 0.3166 - accuracy: 0.8941 - val_loss: 1.1424 - val_accuracy: 0.7507\n",
            "Epoch 58/100\n",
            "105/105 [==============================] - 178s 2s/step - loss: 0.3190 - accuracy: 0.8969 - val_loss: 1.3079 - val_accuracy: 0.7374\n",
            "Epoch 59/100\n",
            "105/105 [==============================] - 179s 2s/step - loss: 0.3398 - accuracy: 0.9018 - val_loss: 1.2214 - val_accuracy: 0.7533\n",
            "Epoch 60/100\n",
            "105/105 [==============================] - 178s 2s/step - loss: 0.3173 - accuracy: 0.8968 - val_loss: 1.2787 - val_accuracy: 0.7241\n",
            "Epoch 61/100\n",
            "105/105 [==============================] - 177s 2s/step - loss: 0.3080 - accuracy: 0.8998 - val_loss: 1.3188 - val_accuracy: 0.7188\n",
            "Epoch 62/100\n",
            "105/105 [==============================] - 178s 2s/step - loss: 0.3551 - accuracy: 0.8871 - val_loss: 1.2147 - val_accuracy: 0.7639\n",
            "Epoch 63/100\n",
            "105/105 [==============================] - 178s 2s/step - loss: 0.3495 - accuracy: 0.8888 - val_loss: 1.4871 - val_accuracy: 0.7056\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fca9532d050>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY6v94A4Bq3U"
      },
      "source": [
        "### 1.6 Unfreeze base model and train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "id": "LvQ84RZreDOU",
        "outputId": "6728c262-fe13-4e3e-a562-bb8da3f70515"
      },
      "source": [
        "base_model = tf.keras.applications.DenseNet121(\n",
        "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
        "    input_shape=input_size_3d,\n",
        "    include_top=False) \n",
        "\n",
        "base_model.trainable = True\n",
        "\n",
        "input_image = tf.keras.Input(shape=input_size_3d)\n",
        "x1 = base_model(input_image, training=True)\n",
        "x2 = tf.keras.layers.Flatten()(x1)\n",
        "out = tf.keras.layers.Dense(3, activation='softmax')(x2)\n",
        "model = tf.keras.Model(inputs = input_image, outputs = out)\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr = 1e-6), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.load_weights(model_name)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 512, 400, 3)]     0         \n",
            "_________________________________________________________________\n",
            "densenet121 (Functional)     (None, 16, 12, 1024)      7037504   \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 196608)            0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 589827    \n",
            "=================================================================\n",
            "Total params: 7,627,331\n",
            "Trainable params: 7,543,683\n",
            "Non-trainable params: 83,648\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-133c097c9624>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2232\u001b[0m             f, self.layers, skip_mismatch=skip_mismatch)\n\u001b[1;32m   2233\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2234\u001b[0;31m         \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2236\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    708\u001b[0m                        str(len(weight_values)) + ' elements.')\n\u001b[1;32m    709\u001b[0m     \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m   \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   3704\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3705\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3706\u001b[0;31m       \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3707\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3708\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m    889\u001b[0m             (\"Cannot assign to variable%s due to variable shape %s and value \"\n\u001b[1;32m    890\u001b[0m              \"shape %s are incompatible\") %\n\u001b[0;32m--> 891\u001b[0;31m             (tensor_name, self._shape, value_tensor.shape))\n\u001b[0m\u001b[1;32m    892\u001b[0m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[1;32m    893\u001b[0m           self.handle, value_tensor, name=name)\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot assign to variable conv2_block1_1_conv/kernel:0 due to variable shape (1, 1, 64, 128) and value shape (64,) are incompatible"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVnCQJfICcd4"
      },
      "source": [
        "model.fit(train_generator, steps_per_epoch=3391/batch_size,\n",
        "          epochs=100, verbose=1,\n",
        "          callbacks=[early_stop, monitor, lr_schedule],\n",
        "          validation_data=val_generator, validation_steps=377/batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCX59QIbn2z_"
      },
      "source": [
        "## 2. Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-1E5hhHn2z_"
      },
      "source": [
        "# You are free to adapt this portion of the code, but you should \n",
        "# compute the test accuracy and show the images that \n",
        "# were classified incorrectly\n",
        "test_data_dir = \"/media/roberto/f5da97cf-b92d-484c-96e9-15766931cebe/Garbage-classification/Dataset-curated/Resized/Test/\"\n",
        "\n",
        "model.load_weights(model_name)\n",
        "\n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1/255.0) \n",
        "test_generator = test_datagen.flow_from_directory(test_data_dir,\n",
        "                                                    target_size=(img_height, img_width),\n",
        "                                                    batch_size= batch_size,shuffle = False)\n",
        "nb_samples = len(glob.glob(test_data_dir + \"*/*\"))\n",
        "model.evaluate(test_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-4jTI1zn20A"
      },
      "source": [
        "img = []\n",
        "true_label = []\n",
        "pred_label = []\n",
        "for ii in range(nb_samples//batch_size):\n",
        "    Xbatch,Ybatch = test_generator.__getitem__(ii)\n",
        "    Ybatch = Ybatch.argmax(axis = 1)\n",
        "    Ypred = model.predict(Xbatch).argmax(axis = 1)\n",
        "    wrong_indexes = np.where(Ypred != Ybatch)[0]\n",
        "    for ii in wrong_indexes:\n",
        "        img.append(Xbatch[ii])\n",
        "        true_label.append(Ybatch[ii])\n",
        "        pred_label.append(Ypred[ii])\n",
        "\n",
        "columns = 4\n",
        "rows = len(img)//columns + 1    \n",
        "plt.figure(figsize = (32,64))\n",
        "for ii in range(len(img)):\n",
        "    plt.subplot(rows,columns,ii+1)\n",
        "    plt.imshow(img[ii], cmap = \"gray\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Label: %s, predicted: %ss\" %(class_names[true_label[ii]]\\\n",
        "                                            ,class_names[pred_label[ii]]))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjtJYZs5n20B"
      },
      "source": [
        "##  Team members participtaion\n",
        "(include the description of what each team member did and the consensus score for each team member)\n",
        "\n",
        "- **Arya Stark** helped design the model and write the code for fully connected model (**score 3**)\n",
        "- **Luke Skywalker** helped design helped to implement the data augmentation module (**score 3**)\n",
        "- ..."
      ]
    }
  ]
}